form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1,2,7:9)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(temp.gma.df[,c(3:6)]),collapse="+")
,collapse=""))
gam3<-gam(formula = form, data = df.train)
summary(gam3)
temp.gma.df<-temp.gma.df[,-6]
form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1,2,6:8)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(temp.gma.df[,c(3:5)]),collapse="+")
,collapse=""))
gam4<-gam(formula = form, data = df.train)
summary(gam4)
rm(form)
# temp.gma.df<-temp.gma.df[,-c(8,9)]
#
# form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1,2,8,9)]),
#                                         ",d=4",")",collapse="+"),
#                         "+",paste0(colnames(temp.gma.df[,c(3:7)]),collapse="+")
#                         ,collapse=""))
# gam.final<-gam(formula = form, data = df.train)
# summary(gam.final)
# rm(form)
form<-as.formula(paste0(resp,"~",paste0("s(",colnames(df.train[,c(2,4,43:45)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(df.train[,c(26,32,34)]),collapse="+")
,collapse=""))
gam.final<-gam(formula = form, data = df.train)
summary(gam.final)
par(mfrow=c(2,4))
plot(gam.final, se = T, col = "blue")
#Now, to decide the degree of freedom for the best gam model.
set.seed(4)
gam.obj<-gam(formula = form,data=df.train)
gam.step<-step.Gam(gam.obj,scope= list("TYPEHUQ"=~1+TYPEHUQ+s(TYPEHUQ,df=2)+s(TYPEHUQ,df=3)+s(TYPEHUQ,df=4)+s(TYPEHUQ,df=5),
"HDD65"=~1+HDD65+s(HDD65,df=2)+s(HDD65,df=3)+s(HDD65,df=4)+s(HDD65,df=5),
"DOOR1SUM"=~1+DOOR1SUM+s(DOOR1SUM,df=2)+s(DOOR1SUM,df=3)+s(DOOR1SUM,df=4)+s(DOOR1SUM,df=5),
"DOLLAREL"=~1+DOLLAREL+s(DOLLAREL,df=2)+s(DOLLAREL,df=3)+s(DOLLAREL,df=4)+s(DOLLAREL,df=5),
"ROOFTYPE"=~1+ROOFTYPE,
"TOTROOMS"=~1+TOTROOMS,
"NUMH2OHTRS"=~1+NUMH2OHTRS,
"WHEATSIZ2"=~1+WHEATSIZ2,
"AIRCOND"=~1+AIRCOND,
"TOTHSQFT"=~1+TOTHSQFT, "TOTCSQFT"=~1+TOTCSQFT), direction = "both",trace=2)
gam.bestmodel<-gam(resp ~ s(TYPEHUQ, d = 4) + s(HDD65, d = 4) + s(TOTCSQFT, d = 4) +s(TOTCSQFT, d = 4)
+ s(DOLLAREL, d = 4) + NUMH2OHTRS + WHEATSIZ2 + ACOTHERS, data = df.train)
gam.pred.OS <- predict(gam.bestmodel,df.test)
error.df$gam[i] <-rmse(actual = df.test$resp,predicted = gam.pred.OS)
gam.diag=data.frame(gam.bestmodel$residuals, gam.bestmodel$fitted.values)
colnames(gam.diag)<-c('resid', 'pred')
plot(x=df.test$resp, y=gam.pred.OS, xlab="Y", ylab="Y-hat", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=gam.bestmodel$residuals, x=gam.bestmodel$fitted.values, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat", xlim = c(0,30000))
qqnorm(gam.bestmodel$residuals)
qqline(gam.bestmodel$residuals)
#CART-----------------------------
set.seed(4)
#Unpruned CART
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[,-1]),collapse="+")))
tree.model1<-rpart(formula = form,data=df.train)
summary(tree.model1)
rpart.plot(tree.model1)
tree.model1.cv<-crossValidate("kfold",10,df.train, tree.model1,"resp")
rm(form)
# tree.model1.predict.OS<-predict(tree.model1,df.test)
# error.df$tree[i] <-rmse(actual = df.test$resp,predicted = tree.model1.predict.OS)
#Pruned CART
plotcp(tree.model1,minline=T,lty=3,col=1,upper=c('size','splits','none'))
tree.model2<-prune(tree.model1, 0.27)
tree.model2.cv<-crossValidate("kfold",10,df.train, tree.model2,"resp")
# tree.model1.predict.OS<-predict(tree.model1,df.test)
# error.df$tree[i] <-rmse(actual = df.test$resp,predicted = tree.model1.predict.OS)
#Model Comparison
RMSEcompare.cart<-data.frame(tree.model1.cv[1],tree.model2.cv[1])
colnames(RMSEcompare.cart)<-c("CART_model1_rmse","CART_model2_rmse")
RMSEcompare.cart
par(mfrow=c(1,1))
boxplot(RMSEcompare.cart, col = c("blue","red"))
legend("bottomright",legend=c("CART_model1_rmse","CART_model2_rmse"),col=c("blue","red"),pch=c(19,19), cex = 0.6)
#The Unpruned model has quite lower out-of-sample rmse value. The variance in the rmse values is almost similar in both cases.
#So, we select the unpruned model.
tree.temp<-data.frame(tree.model1$variable.importance)
form<-as.formula(paste0(resp,"~",paste0(colnames(df.train[,c(2,4,5,7,8,10,11,24,26,38,41,43:45)]),collapse="+")
,collapse=""))
tree.model1<-rpart(formula = form,data=df.train)
rm(form)
tree.model1.predict.OS<-predict(tree.model1, df.test)
error.df$tree[i] <-rmse(actual = df.test$resp,predicted = tree.model1.predict.OS)
tree.resid<-df.test$resp-tree.model1.predict.OS
rpart.diag=data.frame(tree.resid, tree.model1.predict.OS)
colnames(rpart.diag)<-c('resid', 'pred')
plot(x=df.test$resp, y=tree.model1.predict.OS, xlab="Y", ylab="Y-hat", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=tree.resid, x=tree.model1.predict.OS, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat")
qqnorm(tree.resid)
qqline(tree.resid)
#Random Forest-----------------------
set.seed(4)
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[,-1]),collapse="+")))
rf.model<-randomForest(formula = form,data=df.train, importance = TRUE)
rm(form)
# #Using VSURF to do variable selection
# rf.model.vsurf<-VSURF(formula = form,data=df.train, parallel = TRUE, ncores = 20,mtry = 10)
# summary(rf.model.vsurf)
# plot(rf.model.vsurf, step = "thres", imp.sd = FALSE, var.names = TRUE)
varImpPlot(rf.model,sort =TRUE, n.var=min(20, if(is.null(dim(rf.model$importance)))
length(rf.model$importance) else nrow(rf.model$importance)))
#Building model using the important variables derived from %IncMSE graph
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[,c(2:5,7,8,10,11,21,24,34,35,37:41,43:45)])
,collapse="+")))
rf.bestmodel<-randomForest(formula = form,data=df.train, importance = TRUE)
rm(form)
rf.bestmodel.predict.OS<-predict(rf.bestmodel, df.test)
error.df$randomForest[i] <-rmse(actual = df.test$resp,predicted = rf.bestmodel.predict.OS)
rf.resid<-df.train$resp-rf.bestmodel$predicted
resid.rf.OS<-df.test$resp-rf.bestmodel.predict.OS
plot(x=rf.bestmodel.predict.OS,y=df.test$resp,xlab="Y-hat",ylab="Y", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=df.train$resp, x=rf.bestmodel$predicted, xlab="Fitted Values", ylab="Y", main="Y vs. Fitted values")
plot(y=resid.rf.OS, x=rf.bestmodel.predict.OS, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat")
qqnorm(rf.resid)
qqline(rf.resid)
#MARS-------------------------
#First we build an unpruned MARS model
set.seed(4)
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[-1]),collapse="+")))
mars.model1<-earth(formula=form,data=df.train,pmethod="none")
rm(form)
summary(mars.model1)
plotmo(mars.model1, bottom_margin = 1)
mars.model1.cv<-crossValidate("kfold",10,df.train,mars.model1,"resp")
#Now, we compare the model with a pruned MARS model
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[-1]),collapse="+")))
mars.model2<-earth(formula=form,data=df.train)
rm(form)
summary(mars.model2)
plotmo(mars.model2)
mars.model2.cv<-crossValidate("kfold",10,df.train,mars.model2,"resp")
#Comparing the 2 MARS models
RMSE<-data.frame(mars.model1.cv[1],mars.model2.cv[1])
colnames(RMSE)<-c("MARS_model1_rmse","MARS_model2_rmse")
RMSE
par(mfrow=c(1,1))
boxplot(RMSE, col = c("blue","red"))
legend("bottomright",legend=c("Model1_MARS","Model2_MARS"),col=c("blue","red"),pch=c(19,19), cex = 0.6)
#Since, the pruned MARS model has lower mean rmseOS value and also the standard deviation rmseOS values is lesser.
#Therefore, we select the pruned MARS model.
mars.finalmodel.predict.OS<-predict(mars.model2, df.test)
error.df$MARS[i] <-rmse(actual = df.test$resp,predicted = mars.finalmodel.predict.OS)
mars.resid<-mars.model2$residuals
mars.fitted<-mars.model2$fitted.values
plot(x=mars.finalmodel.predict.OS,y=df.test$resp,xlab="Y-hat",ylab="Y", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=df.train$resp, x=mars.fitted, xlab="Fitted Values", ylab="Y", main="Y vs. Fitted values")
plot(y=mars.resid, x=mars.fitted, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat",xlim = c(0,30000))
qqnorm(mars.resid)
qqline(mars.resid)
#BART-------------------------
# options(java.parameters="-Xmx5g")
# library('bartMachine')
# library('rJava')
# set_bart_machine_num_cores(20)
set.seed(4)
df.train.covariates <- df.train[,-1]
df.train.response <-df.train$resp
bart.df<- bartMachine(X=df.train.covariates,y=df.train.response, serialize = TRUE)
bart.pred.OS <- predict(bart.df,new_data = df.test[,-1])
error.df$bart[i] <-rmse(actual=df.test$resp,predicted = bart.pred.OS)
par(mfrow=c(1,1))
investigate_var_importance(bart.df, num_replicates_for_avg = 20)
plot_y_vs_yhat(bart.df, credible_intervals = TRUE)
# plot_y_vs_yhat(bart.df, prediction_intervals = TRUE)
pd_plot(bart.df,j='DOLLAREL')
pd_plot(bart.df,j='TOTCSQFT')
pd_plot(bart.df,j='TOTHSQFT')
pd_plot(bart.df,j='HDD65')
check_bart_error_assumptions(bart.df, hetero_plot = "yhats")
# var_select_bartcv<-var_selection_by_permute_cv(bart.df, k_folds=10,
#                                                num_reps_for_avg=5,num_trees_for_permute=20, alpha=0.05, num_trees_pred_cv=50)
#
# print(var_select_bartcv$best_method)
# print(var_select_bartcv$important_vars_cv)
#
#SVM-----------------------------
set.seed(4)
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[-1]),collapse="+")))
svm.model<-svm(formula=form,data=df.train,cost = 1000, gamma = 0.0001)
rm(form)
summary(svm.model)
svm.model.predict.OS<-predict(svm.model, df.test)
error.df$svm[i] <-rmse(actual=df.test$resp,predicted = svm.model.predict.OS)
svm.resid<-svm.model$residuals
svm.fitted<-svm.model$fitted
plot(x=svm.model.predict.OS,y=df.test$resp,xlab="Y-hat",ylab="Y", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=df.train$resp, x=svm.fitted, xlab="Fitted Values", ylab="Y", main="Y vs. Fitted values")
plot(y=svm.resid, x=svm.fitted, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat",xlim = c(0,30000))
qqnorm(svm.resid)
qqline(svm.resid)
}
library(ModelMetrics)
library(gam)
library(earth)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
set.seed(4)
folds <- 10
df$folds <- sample(seq(1:folds),size=nrow(df),replace=T)
error.df <- data.frame(glm=numeric(folds),gam=numeric(folds),tree=numeric(folds),
randomForest=numeric(folds),MARS=numeric(folds),bart=numeric(folds),svm=numeric(folds))
for(i in (1:folds)){
# start a for loop
set.seed(4)
df.test<- df[which(df$folds==i),]
df.train <-df[-which(df$folds==i),]
#GLM-----------------------------------
glm1<-glm(resp~.,data = df.train, family = gaussian)
summary(glm1)
glm2<-glm(resp ~ HDD65+TOTROOMS+CELLAR+GARGHEAT+GARGCOOL+WHEATSIZ2+ACROOMS+
USEWWAC+WINDOWS+TOTCSQFT+DOLLAREL ,data = df.train)
summary(glm2)
glm2.pred.OS <- predict(glm2,df.test)
error.df$glm[i] <-rmse(actual = df.test$resp,predicted = glm2.pred.OS)
glm.diag=data.frame(glm2$residuals, glm2$fitted.values)
colnames(glm.diag)<-c('resid', 'pred')
plot(x=df.test$resp, y=glm2.pred.OS, xlab="Y", ylab="Y-hat", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=glm2$residuals, x=glm2$fitted.values, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat", xlim = c(0,30000))
qqnorm(glm2$residuals)
qqline(glm2$residuals)
#GAM-------------------------------------
resp<-"resp"
gam1<-gam(resp~.,data=df.train)
summary(gam1)
temp.gma.df<-df.train[,-c(1,5:7,12,17:19,21,23,24,27,29,33,38)]
form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1:7,17,21,23,24,26:30)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(temp.gma.df[,c(8:16,18:20,22,25)]),collapse="+")
,collapse=""))
gam2<-gam(formula = form, data = df.train)
summary(gam2)
rm(form)
temp.gma.df<-temp.gma.df[,c(1,3,15,19,20,22,28:30)]
form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1,2,7:9)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(temp.gma.df[,c(3:6)]),collapse="+")
,collapse=""))
gam3<-gam(formula = form, data = df.train)
summary(gam3)
temp.gma.df<-temp.gma.df[,-6]
form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1,2,6:8)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(temp.gma.df[,c(3:5)]),collapse="+")
,collapse=""))
gam4<-gam(formula = form, data = df.train)
summary(gam4)
rm(form)
# temp.gma.df<-temp.gma.df[,-c(8,9)]
#
# form<-as.formula(paste0(resp,"~",paste0("s(",colnames(temp.gma.df[,c(1,2,8,9)]),
#                                         ",d=4",")",collapse="+"),
#                         "+",paste0(colnames(temp.gma.df[,c(3:7)]),collapse="+")
#                         ,collapse=""))
# gam.final<-gam(formula = form, data = df.train)
# summary(gam.final)
# rm(form)
form<-as.formula(paste0(resp,"~",paste0("s(",colnames(df.train[,c(2,4,43:45)]),
",d=4",")",collapse="+"),
"+",paste0(colnames(df.train[,c(26,32,34)]),collapse="+")
,collapse=""))
gam.final<-gam(formula = form, data = df.train)
summary(gam.final)
par(mfrow=c(2,4))
plot(gam.final, se = T, col = "blue")
#Now, to decide the degree of freedom for the best gam model.
set.seed(4)
gam.obj<-gam(formula = form,data=df.train)
gam.step<-step.Gam(gam.obj,scope= list("TYPEHUQ"=~1+TYPEHUQ+s(TYPEHUQ,df=2)+s(TYPEHUQ,df=3)+s(TYPEHUQ,df=4)+s(TYPEHUQ,df=5),
"HDD65"=~1+HDD65+s(HDD65,df=2)+s(HDD65,df=3)+s(HDD65,df=4)+s(HDD65,df=5),
"DOOR1SUM"=~1+DOOR1SUM+s(DOOR1SUM,df=2)+s(DOOR1SUM,df=3)+s(DOOR1SUM,df=4)+s(DOOR1SUM,df=5),
"DOLLAREL"=~1+DOLLAREL+s(DOLLAREL,df=2)+s(DOLLAREL,df=3)+s(DOLLAREL,df=4)+s(DOLLAREL,df=5),
"ROOFTYPE"=~1+ROOFTYPE,
"TOTROOMS"=~1+TOTROOMS,
"NUMH2OHTRS"=~1+NUMH2OHTRS,
"WHEATSIZ2"=~1+WHEATSIZ2,
"AIRCOND"=~1+AIRCOND,
"TOTHSQFT"=~1+TOTHSQFT, "TOTCSQFT"=~1+TOTCSQFT), direction = "both",trace=2)
gam.bestmodel<-gam(resp ~ s(TYPEHUQ, d = 4) + s(HDD65, d = 4) + s(TOTCSQFT, d = 4) +s(TOTCSQFT, d = 4)
+ s(DOLLAREL, d = 4) + NUMH2OHTRS + WHEATSIZ2 + ACOTHERS, data = df.train)
gam.pred.OS <- predict(gam.bestmodel,df.test)
error.df$gam[i] <-rmse(actual = df.test$resp,predicted = gam.pred.OS)
gam.diag=data.frame(gam.bestmodel$residuals, gam.bestmodel$fitted.values)
colnames(gam.diag)<-c('resid', 'pred')
plot(x=df.test$resp, y=gam.pred.OS, xlab="Y", ylab="Y-hat", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=gam.bestmodel$residuals, x=gam.bestmodel$fitted.values, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat", xlim = c(0,30000))
qqnorm(gam.bestmodel$residuals)
qqline(gam.bestmodel$residuals)
#CART-----------------------------
set.seed(4)
#Unpruned CART
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[,-1]),collapse="+")))
tree.model1<-rpart(formula = form,data=df.train)
summary(tree.model1)
rpart.plot(tree.model1)
tree.model1.cv<-crossValidate("kfold",10,df.train, tree.model1,"resp")
rm(form)
# tree.model1.predict.OS<-predict(tree.model1,df.test)
# error.df$tree[i] <-rmse(actual = df.test$resp,predicted = tree.model1.predict.OS)
#Pruned CART
plotcp(tree.model1,minline=T,lty=3,col=1,upper=c('size','splits','none'))
tree.model2<-prune(tree.model1, 0.27)
tree.model2.cv<-crossValidate("kfold",10,df.train, tree.model2,"resp")
# tree.model1.predict.OS<-predict(tree.model1,df.test)
# error.df$tree[i] <-rmse(actual = df.test$resp,predicted = tree.model1.predict.OS)
#Model Comparison
RMSEcompare.cart<-data.frame(tree.model1.cv[1],tree.model2.cv[1])
colnames(RMSEcompare.cart)<-c("CART_model1_rmse","CART_model2_rmse")
RMSEcompare.cart
par(mfrow=c(1,1))
boxplot(RMSEcompare.cart, col = c("blue","red"))
legend("bottomright",legend=c("CART_model1_rmse","CART_model2_rmse"),col=c("blue","red"),pch=c(19,19), cex = 0.6)
#The Unpruned model has quite lower out-of-sample rmse value. The variance in the rmse values is almost similar in both cases.
#So, we select the unpruned model.
tree.temp<-data.frame(tree.model1$variable.importance)
form<-as.formula(paste0(resp,"~",paste0(colnames(df.train[,c(2,4,5,7,8,10,11,24,26,38,41,43:45)]),collapse="+")
,collapse=""))
tree.model1<-rpart(formula = form,data=df.train)
rm(form)
tree.model1.predict.OS<-predict(tree.model1, df.test)
error.df$tree[i] <-rmse(actual = df.test$resp,predicted = tree.model1.predict.OS)
tree.resid<-df.test$resp-tree.model1.predict.OS
rpart.diag=data.frame(tree.resid, tree.model1.predict.OS)
colnames(rpart.diag)<-c('resid', 'pred')
plot(x=df.test$resp, y=tree.model1.predict.OS, xlab="Y", ylab="Y-hat", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=tree.resid, x=tree.model1.predict.OS, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat")
qqnorm(tree.resid)
qqline(tree.resid)
#Random Forest-----------------------
set.seed(4)
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[,-1]),collapse="+")))
rf.model<-randomForest(formula = form,data=df.train, importance = TRUE)
rm(form)
# #Using VSURF to do variable selection
# rf.model.vsurf<-VSURF(formula = form,data=df.train, parallel = TRUE, ncores = 20,mtry = 10)
# summary(rf.model.vsurf)
# plot(rf.model.vsurf, step = "thres", imp.sd = FALSE, var.names = TRUE)
varImpPlot(rf.model,sort =TRUE, n.var=min(20, if(is.null(dim(rf.model$importance)))
length(rf.model$importance) else nrow(rf.model$importance)))
#Building model using the important variables derived from %IncMSE graph
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[,c(2:5,7,8,10,11,21,24,34,35,37:41,43:45)])
,collapse="+")))
rf.bestmodel<-randomForest(formula = form,data=df.train, importance = TRUE)
rm(form)
rf.bestmodel.predict.OS<-predict(rf.bestmodel, df.test)
error.df$randomForest[i] <-rmse(actual = df.test$resp,predicted = rf.bestmodel.predict.OS)
rf.resid<-df.train$resp-rf.bestmodel$predicted
resid.rf.OS<-df.test$resp-rf.bestmodel.predict.OS
plot(x=rf.bestmodel.predict.OS,y=df.test$resp,xlab="Y-hat",ylab="Y", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=df.train$resp, x=rf.bestmodel$predicted, xlab="Fitted Values", ylab="Y", main="Y vs. Fitted values")
plot(y=resid.rf.OS, x=rf.bestmodel.predict.OS, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat")
qqnorm(rf.resid)
qqline(rf.resid)
#MARS-------------------------
#First we build an unpruned MARS model
set.seed(4)
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[-1]),collapse="+")))
mars.model1<-earth(formula=form,data=df.train,pmethod="none")
rm(form)
summary(mars.model1)
plotmo(mars.model1, bottom_margin = 1)
mars.model1.cv<-crossValidate("kfold",10,df.train,mars.model1,"resp")
#Now, we compare the model with a pruned MARS model
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[-1]),collapse="+")))
mars.model2<-earth(formula=form,data=df.train)
rm(form)
summary(mars.model2)
plotmo(mars.model2)
mars.model2.cv<-crossValidate("kfold",10,df.train,mars.model2,"resp")
#Comparing the 2 MARS models
RMSE<-data.frame(mars.model1.cv[1],mars.model2.cv[1])
colnames(RMSE)<-c("MARS_model1_rmse","MARS_model2_rmse")
RMSE
par(mfrow=c(1,1))
boxplot(RMSE, col = c("blue","red"))
legend("bottomright",legend=c("Model1_MARS","Model2_MARS"),col=c("blue","red"),pch=c(19,19), cex = 0.6)
#Since, the pruned MARS model has lower mean rmseOS value and also the standard deviation rmseOS values is lesser.
#Therefore, we select the pruned MARS model.
mars.finalmodel.predict.OS<-predict(mars.model2, df.test)
error.df$MARS[i] <-rmse(actual = df.test$resp,predicted = mars.finalmodel.predict.OS)
mars.resid<-mars.model2$residuals
mars.fitted<-mars.model2$fitted.values
plot(x=mars.finalmodel.predict.OS,y=df.test$resp,xlab="Y-hat",ylab="Y", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=df.train$resp, x=mars.fitted, xlab="Fitted Values", ylab="Y", main="Y vs. Fitted values")
plot(y=mars.resid, x=mars.fitted, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat",xlim = c(0,30000))
qqnorm(mars.resid)
qqline(mars.resid)
#BART-------------------------
# options(java.parameters="-Xmx5g")
# library('bartMachine')
# library('rJava')
# set_bart_machine_num_cores(20)
set.seed(4)
df.train.covariates <- df.train[,-1]
df.train.response <-df.train$resp
bart.df<- bartMachine(X=df.train.covariates,y=df.train.response, serialize = TRUE)
bart.pred.OS <- predict(bart.df,new_data = df.test[,-1])
error.df$bart[i] <-rmse(actual=df.test$resp,predicted = bart.pred.OS)
par(mfrow=c(1,1))
investigate_var_importance(bart.df, num_replicates_for_avg = 20)
plot_y_vs_yhat(bart.df, credible_intervals = TRUE)
# plot_y_vs_yhat(bart.df, prediction_intervals = TRUE)
pd_plot(bart.df,j='DOLLAREL')
pd_plot(bart.df,j='TOTCSQFT')
pd_plot(bart.df,j='TOTHSQFT')
pd_plot(bart.df,j='HDD65')
check_bart_error_assumptions(bart.df, hetero_plot = "yhats")
# var_select_bartcv<-var_selection_by_permute_cv(bart.df, k_folds=10,
#                                                num_reps_for_avg=5,num_trees_for_permute=20, alpha=0.05, num_trees_pred_cv=50)
#
# print(var_select_bartcv$best_method)
# print(var_select_bartcv$important_vars_cv)
#
#SVM-----------------------------
set.seed(4)
form<-as.formula(paste0(names(df.train[1]),"~",paste0(names(df.train[-1]),collapse="+")))
svm.model<-svm(formula=form,data=df.train,cost = 1000, gamma = 0.0001)
rm(form)
summary(svm.model)
svm.model.predict.OS<-predict(svm.model, df.test)
error.df$svm[i] <-rmse(actual=df.test$resp,predicted = svm.model.predict.OS)
svm.resid<-svm.model$residuals
svm.fitted<-svm.model$fitted
plot(x=svm.model.predict.OS,y=df.test$resp,xlab="Y-hat",ylab="Y", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=df.train$resp, x=svm.fitted, xlab="Fitted Values", ylab="Y", main="Y vs. Fitted values")
plot(y=svm.resid, x=svm.fitted, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat",xlim = c(0,30000))
qqnorm(svm.resid)
qqline(svm.resid)
}
View(error.df)
View(error.df)
error.df<-data.frame(mean(error.df$glm[i]),mean(error.df$gam[i]),mean(error.df$tree[i]),
mean(error.df$randomForest[i]),mean(error.df$MARS[i]),mean(error.df$bart[i]),
mean(error.df$svm[i]))
View(error.df)
View(error.df)
gam.diag=data.frame(gam.bestmodel$residuals, gam.bestmodel$fitted.values)
colnames(gam.diag)<-c('resid', 'pred')
plot(x=df.test$resp, y=gam.pred.OS, xlab="Y", ylab="Y-hat", main="Y-hat vs. Y")# Y vs. Y-hat
plot(y=gam.bestmodel$residuals, x=gam.bestmodel$fitted.values, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat", xlim = c(0,30000))
qqnorm(gam.bestmodel$residuals)
qqline(gam.bestmodel$residuals)
gam.diag=data.frame(gam.bestmodel$residuals, gam.bestmodel$fitted.values)
colnames(gam.diag)<-c('resid', 'pred')
plot(y=df.test$resp, x=gam.pred.OS, ylab="Y", xlab="Y-hat", main="Y vs. Y-hat")# Y vs. Y-hat
plot(y=gam.bestmodel$residuals, x=gam.bestmodel$fitted.values, xlab="Fitted Values", ylab="Residuals", main="e vs. Y-hat", xlim = c(0,30000))
qqnorm(gam.bestmodel$residuals)
qqline(gam.bestmodel$residuals)
finalmodel<-gam(resp ~ s(TYPEHUQ, d = 4) + s(HDD65, d = 4) + s(TOTCSQFT, d = 4) +s(TOTCSQFT, d = 4)
+ s(DOLLAREL, d = 4) + NUMH2OHTRS + WHEATSIZ2 + ACOTHERS, data = df.train)
finalmodel<-gam(resp ~ s(TYPEHUQ, d = 4) + s(HDD65, d = 4) + s(TOTCSQFT, d = 4) +s(TOTCSQFT, d = 4)
+ s(DOLLAREL, d = 4) + NUMH2OHTRS + WHEATSIZ2 + ACOTHERS, data = df.train)
save.image("~/FINAL/shah398.RData")
load("~/Harmanik/Analysis2.RData")
pd<-pd[c(3:6,10,12:15)]
pd<-names(df.train)
pd<-pd[c(3:6,10,12:15)]
pd
print(i)
for(i in pd){
print(i)
png(file=paste0(i,'_pd.png'), width= 10, height = 10, units = 'in', res = 300)
pd_plot(bart_machine_cv,j=i)
dev.off()
cat('\014')
}
save.image("~/Harmanik/Analysis2.RData")
for(i in pd){
print(i)
png(file=paste0(i,'_pd.png'), width= 10, height = 10, units = 'in', res = 300)
pd_plot(bart_machine_cv,j=i)
dev.off()
cat('\014')
}
bartModel <- bartMachine(X, Y, use_missing_data = TRUE,serialize = T)
.libPaths( c( .libPaths(), "/home/sonin/Rlibs") )
options(java.parameters="-Xmx100g")
library('bartMachine')
library('rJava')
set_bart_machine_num_cores(20)
bartMachine()
for(i in pd){
print(i)
png(file=paste0(i,'_pd.png'), width= 10, height = 10, units = 'in', res = 300)
pd_plot(bart_machine_cv,j=i)
dev.off()
cat('\014')
}
pd
pd
pd<-pd[-8]
pd
pd<-pd['Events_2']
pd
pd<-names(df.train)
pd<-pd[c(3:6,10,12:15)]
pd
pd<-pd[-8]
pd_plot(bart_machine_cv,j='Events_2')
pd_plot(bart_machine_cv,j='Events_2')
pd_plot(bart_machine_cv,j='GC_0')
gam3
save.image("~/Harmanik/Analysis2.RData")
resp<-"PM2.5"
wd<-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(wd)
save.image("~/Harmanik/Analysis2.RData")
